{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a81b8de8-be89-4d13-a1d1-4880d2383cc1",
   "metadata": {},
   "source": [
    "# Summary "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e02ce4-2178-48e2-befa-bfa0f47ed6ca",
   "metadata": {},
   "source": [
    "This project is a sample chatbot project that provide answers to Computer Science domain questions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c66e2f-27ed-4031-98c9-55705031954b",
   "metadata": {},
   "source": [
    "# Checking CUDA availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc1b7533-3d28-42ca-8907-1c23cba5f5be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: False\n",
      "CUDA Version: None\n",
      "Device Name: CPU\n",
      "Torch Version: 2.5.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"CUDA Version:\", torch.version.cuda)\n",
    "print(\"Device Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n",
    "print(\"Torch Version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef2c3bd-e653-42e5-909b-c963053d4827",
   "metadata": {},
   "source": [
    "# Installing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8e6b34e-f7bb-4300-995b-891b0a0f1b87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\dell\\anaconda3\\lib\\site-packages (3.8.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (1.0.11)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (8.3.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (2.5.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (0.15.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (4.66.5)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (2.8.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.12.14)\n",
      "Requirement already satisfied: blis<1.2.0,>=1.1.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.0->spacy) (1.1.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.0->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\dell\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\dell\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\dell\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dell\\anaconda3\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: transformers in c:\\users\\dell\\anaconda3\\lib\\site-packages (4.47.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\dell\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from transformers) (0.27.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\dell\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.12.14)\n",
      "Requirement already satisfied: torch in c:\\users\\dell\\anaconda3\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\dell\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\dell\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\dell\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\dell\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: streamlit in c:\\users\\dell\\anaconda3\\lib\\site-packages (1.43.0)\n",
      "Requirement already satisfied: altair<6,>=4.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from streamlit) (5.0.1)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from streamlit) (1.6.2)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from streamlit) (5.3.3)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from streamlit) (8.1.7)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from streamlit) (1.26.4)\n",
      "Requirement already satisfied: packaging<25,>=20 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from streamlit) (24.1)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from streamlit) (2.2.2)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from streamlit) (10.4.0)\n",
      "Requirement already satisfied: protobuf<6,>=3.20 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from streamlit) (4.25.3)\n",
      "Requirement already satisfied: pyarrow>=7.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from streamlit) (16.1.0)\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from streamlit) (2.32.3)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from streamlit) (8.2.3)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from streamlit) (4.11.0)\n",
      "Requirement already satisfied: watchdog<7,>=2.1.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from streamlit) (4.0.1)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from streamlit) (3.1.43)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from streamlit) (0.8.0)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from streamlit) (6.4.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
      "Requirement already satisfied: toolz in c:\\users\\dell\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (0.12.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\anaconda3\\lib\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2024.12.14)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.10.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.16.0)\n",
      "Requirement already satisfied: evaluate in c:\\users\\dell\\anaconda3\\lib\\site-packages (0.4.3)\n",
      "Requirement already satisfied: datasets>=2.0.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from evaluate) (3.2.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in c:\\users\\dell\\anaconda3\\lib\\site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\dell\\anaconda3\\lib\\site-packages (from evaluate) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from evaluate) (4.66.5)\n",
      "Requirement already satisfied: xxhash in c:\\users\\dell\\anaconda3\\lib\\site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\dell\\anaconda3\\lib\\site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from evaluate) (0.27.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\dell\\anaconda3\\lib\\site-packages (from evaluate) (24.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\dell\\anaconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (16.1.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\dell\\anaconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.10.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (2024.12.14)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tqdm>=4.62.1->evaluate) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas->evaluate) (2023.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.11.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\dell\\anaconda3\\lib\\site-packages (7.8.1)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=3.6.6 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from ipywidgets) (3.6.6)\n",
      "Requirement already satisfied: ipython>=4.0.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from ipywidgets) (8.27.0)\n",
      "Requirement already satisfied: jupyterlab-widgets<3,>=1.0.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from ipywidgets) (1.0.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\dell\\anaconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\dell\\anaconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (2.15.1)\n",
      "Requirement already satisfied: stack-data in c:\\users\\dell\\anaconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\anaconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: notebook>=4.4.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from widgetsnbextension~=3.6.6->ipywidgets) (7.2.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2.14.1)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.27.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2.27.3)\n",
      "Requirement already satisfied: jupyterlab<4.3,>=4.2.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (4.2.5)\n",
      "Requirement already satisfied: notebook-shim<0.3,>=0.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: tornado>=6.2.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (6.4.1)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\dell\\anaconda3\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=4.0.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: executing in c:\\users\\dell\\anaconda3\\lib\\site-packages (from stack-data->ipython>=4.0.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: asttokens in c:\\users\\dell\\anaconda3\\lib\\site-packages (from stack-data->ipython>=4.0.0->ipywidgets) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\dell\\anaconda3\\lib\\site-packages (from stack-data->ipython>=4.0.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: anyio>=3.1.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (4.2.0)\n",
      "Requirement already satisfied: argon2-cffi>=21.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (21.3.0)\n",
      "Requirement already satisfied: jinja2>=3.0.3 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (3.1.4)\n",
      "Requirement already satisfied: jupyter-client>=7.4.4 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (8.6.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (5.7.2)\n",
      "Requirement already satisfied: jupyter-events>=0.9.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (0.10.0)\n",
      "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (0.4.4)\n",
      "Requirement already satisfied: nbconvert>=6.4.4 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (7.16.4)\n",
      "Requirement already satisfied: nbformat>=5.3.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (5.10.4)\n",
      "Requirement already satisfied: overrides>=5.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (7.4.0)\n",
      "Requirement already satisfied: packaging>=22.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (24.1)\n",
      "Requirement already satisfied: prometheus-client>=0.9 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (0.14.1)\n",
      "Requirement already satisfied: pywinpty>=2.0.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2.0.10)\n",
      "Requirement already satisfied: pyzmq>=24 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (25.1.2)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (1.8.2)\n",
      "Requirement already satisfied: terminado>=0.8.3 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (0.17.1)\n",
      "Requirement already satisfied: websocket-client>=1.7 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (1.8.0)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from jupyterlab<4.3,>=4.2.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2.0.4)\n",
      "Requirement already satisfied: httpx>=0.25.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from jupyterlab<4.3,>=4.2.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (0.27.0)\n",
      "Requirement already satisfied: ipykernel>=6.5.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from jupyterlab<4.3,>=4.2.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (6.28.0)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from jupyterlab<4.3,>=4.2.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: setuptools>=40.1.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from jupyterlab<4.3,>=4.2.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (75.1.0)\n",
      "Requirement already satisfied: babel>=2.10 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2.11.0)\n",
      "Requirement already satisfied: json5>=0.9.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (0.9.6)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (4.23.0)\n",
      "Requirement already satisfied: requests>=2.31 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2.32.3)\n",
      "Requirement already satisfied: six in c:\\users\\dell\\anaconda3\\lib\\site-packages (from asttokens->stack-data->ipython>=4.0.0->ipywidgets) (1.16.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (1.3.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings in c:\\users\\dell\\anaconda3\\lib\\site-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (21.2.0)\n",
      "Requirement already satisfied: pytz>=2015.7 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from babel>=2.10->jupyterlab-server<3,>=2.27.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2024.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\dell\\anaconda3\\lib\\site-packages (from httpx>=0.25.0->jupyterlab<4.3,>=4.2.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\dell\\anaconda3\\lib\\site-packages (from httpx>=0.25.0->jupyterlab<4.3,>=4.2.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab<4.3,>=4.2.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (0.14.0)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (1.6.7)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\dell\\anaconda3\\lib\\site-packages (from ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (1.6.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\dell\\anaconda3\\lib\\site-packages (from ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (5.9.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from jinja2>=3.0.3->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2.1.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (24.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (0.10.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from jupyter-client>=7.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (3.10.0)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (305.1)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2.0.7)\n",
      "Requirement already satisfied: pyyaml>=5.3 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (6.0.1)\n",
      "Requirement already satisfied: rfc3339-validator in c:\\users\\dell\\anaconda3\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (0.1.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (4.12.3)\n",
      "Requirement already satisfied: bleach!=5.0.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (4.1.0)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\dell\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\users\\dell\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (0.1.2)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2.0.4)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (0.8.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (1.5.0)\n",
      "Requirement already satisfied: tinycss2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (1.2.1)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2.16.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (1.26.20)\n",
      "Requirement already satisfied: webencodings in c:\\users\\dell\\anaconda3\\lib\\site-packages (from bleach!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (0.5.1)\n",
      "Requirement already satisfied: fqdn in c:\\users\\dell\\anaconda3\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (1.5.1)\n",
      "Requirement already satisfied: isoduration in c:\\users\\dell\\anaconda3\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2.1)\n",
      "Requirement already satisfied: uri-template in c:\\users\\dell\\anaconda3\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=24.6.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (24.11.1)\n",
      "Requirement already satisfied: cffi>=1.0.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (1.17.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2.5)\n",
      "Requirement already satisfied: pycparser in c:\\users\\dell\\anaconda3\\lib\\site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2.21)\n",
      "Requirement already satisfied: arrow>=0.15.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (1.2.3)\n",
      "Requirement already satisfied: pyrebase4 in c:\\users\\dell\\anaconda3\\lib\\site-packages (4.8.0)\n",
      "Requirement already satisfied: requests-toolbelt<1.0,>=0.7.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pyrebase4) (0.10.1)\n",
      "Requirement already satisfied: requests>=2.31 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pyrebase4) (2.32.3)\n",
      "Requirement already satisfied: urllib3<2,>=1.21.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pyrebase4) (1.26.20)\n",
      "Requirement already satisfied: gcloud>=0.18.3 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pyrebase4) (0.18.3)\n",
      "Requirement already satisfied: oauth2client>=4.1.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pyrebase4) (4.1.3)\n",
      "Requirement already satisfied: python-jwt>=2.0.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pyrebase4) (4.1.0)\n",
      "Requirement already satisfied: pycryptodome>=3.6.4 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pyrebase4) (3.21.0)\n",
      "Requirement already satisfied: httplib2>=0.9.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from gcloud>=0.18.3->pyrebase4) (0.22.0)\n",
      "Requirement already satisfied: googleapis-common-protos in c:\\users\\dell\\anaconda3\\lib\\site-packages (from gcloud>=0.18.3->pyrebase4) (1.69.1)\n",
      "Requirement already satisfied: protobuf!=3.0.0.b2.post1,>=3.0.0b2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from gcloud>=0.18.3->pyrebase4) (4.25.3)\n",
      "Requirement already satisfied: six in c:\\users\\dell\\anaconda3\\lib\\site-packages (from gcloud>=0.18.3->pyrebase4) (1.16.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from oauth2client>=4.1.2->pyrebase4) (0.4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from oauth2client>=4.1.2->pyrebase4) (0.2.8)\n",
      "Requirement already satisfied: rsa>=3.1.4 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from oauth2client>=4.1.2->pyrebase4) (4.9)\n",
      "Requirement already satisfied: jwcrypto>=1.4.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from python-jwt>=2.0.1->pyrebase4) (1.5.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests>=2.31->pyrebase4) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests>=2.31->pyrebase4) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests>=2.31->pyrebase4) (2024.12.14)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from httplib2>=0.9.1->gcloud>=0.18.3->pyrebase4) (3.1.2)\n",
      "Requirement already satisfied: cryptography>=3.4 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from jwcrypto>=1.4.2->python-jwt>=2.0.1->pyrebase4) (43.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from jwcrypto>=1.4.2->python-jwt>=2.0.1->pyrebase4) (4.11.0)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from cryptography>=3.4->jwcrypto>=1.4.2->python-jwt>=2.0.1->pyrebase4) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\dell\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=3.4->jwcrypto>=1.4.2->python-jwt>=2.0.1->pyrebase4) (2.21)\n",
      "Requirement already satisfied: streamlit in c:\\users\\dell\\anaconda3\\lib\\site-packages (1.43.0)\n",
      "Requirement already satisfied: altair<6,>=4.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from streamlit) (5.0.1)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from streamlit) (1.6.2)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from streamlit) (5.3.3)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from streamlit) (8.1.7)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from streamlit) (1.26.4)\n",
      "Requirement already satisfied: packaging<25,>=20 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from streamlit) (24.1)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from streamlit) (2.2.2)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from streamlit) (10.4.0)\n",
      "Requirement already satisfied: protobuf<6,>=3.20 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from streamlit) (4.25.3)\n",
      "Requirement already satisfied: pyarrow>=7.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from streamlit) (16.1.0)\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from streamlit) (2.32.3)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from streamlit) (8.2.3)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from streamlit) (4.11.0)\n",
      "Requirement already satisfied: watchdog<7,>=2.1.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from streamlit) (4.0.1)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from streamlit) (3.1.43)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from streamlit) (0.8.0)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from streamlit) (6.4.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
      "Requirement already satisfied: toolz in c:\\users\\dell\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (0.12.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\anaconda3\\lib\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2024.12.14)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.10.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# For NLP libraries\n",
    "!pip install spacy\n",
    "!pip install nltk\n",
    "!pip install transformers\n",
    "!pip install torch\n",
    "\n",
    "# For Streamlit (optional for frontend)\n",
    "!pip install streamlit\n",
    "\n",
    "!pip install evaluate\n",
    "!pip install ipywidgets\n",
    "!pip install pyrebase4\n",
    "!pip install --upgrade streamlit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c299013-0c06-4bf5-84ce-a97ae239603a",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2937b31a-e171-49f1-9692-6fbb0a2a1cb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "import nltk\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import streamlit\n",
    "import os\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "import torch.optim as optim\n",
    "from evaluate import load\n",
    "import glob\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset\n",
    "import evaluate \n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fa1692-7acd-47f6-a485-3331e1917dbc",
   "metadata": {},
   "source": [
    "# Viewing Dataframe and convering to hugging face dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a050004-e884-4172-9f88-7a1d49ec3de5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7240</th>\n",
       "      <td>How do you rename a directory using the mv com...</td>\n",
       "      <td>Rename a directory with `mv oldfolder newfolde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10078</th>\n",
       "      <td>What are the operations of DSU?</td>\n",
       "      <td>DSU primarily supports two operations: 'find',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14972</th>\n",
       "      <td>What are Features of Bitmap Indexing?</td>\n",
       "      <td>They are space efficient for columns with low ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3578</th>\n",
       "      <td>What is a choropleth map?</td>\n",
       "      <td>A choropleth map uses shade shading or styles ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>What platforms are supported by Elastic Beanst...</td>\n",
       "      <td>Elastic Beanstalk supports various platforms f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10008</th>\n",
       "      <td>What is a doubly linked list?</td>\n",
       "      <td>A doubly linked list is a type of linked list ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17267</th>\n",
       "      <td>How to implement the Stack data structure in J...</td>\n",
       "      <td>The Stack data structure is a linear data stru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16346</th>\n",
       "      <td>How can AI contribute to the creation of perso...</td>\n",
       "      <td>AI can analyze sensory preferences, recommend ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11187</th>\n",
       "      <td>What is segmentation in an operating system?</td>\n",
       "      <td>Segmentation divides a programs memory into l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7427</th>\n",
       "      <td>How do you use a for loop to iterate through a...</td>\n",
       "      <td>\"Create an array or list, then use `for` follo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                question  \\\n",
       "7240   How do you rename a directory using the mv com...   \n",
       "10078                    What are the operations of DSU?   \n",
       "14972              What are Features of Bitmap Indexing?   \n",
       "3578                           What is a choropleth map?   \n",
       "294    What platforms are supported by Elastic Beanst...   \n",
       "10008                      What is a doubly linked list?   \n",
       "17267  How to implement the Stack data structure in J...   \n",
       "16346  How can AI contribute to the creation of perso...   \n",
       "11187       What is segmentation in an operating system?   \n",
       "7427   How do you use a for loop to iterate through a...   \n",
       "\n",
       "                                                  answer  \n",
       "7240   Rename a directory with `mv oldfolder newfolde...  \n",
       "10078  DSU primarily supports two operations: 'find',...  \n",
       "14972  They are space efficient for columns with low ...  \n",
       "3578   A choropleth map uses shade shading or styles ...  \n",
       "294    Elastic Beanstalk supports various platforms f...  \n",
       "10008  A doubly linked list is a type of linked list ...  \n",
       "17267  The Stack data structure is a linear data stru...  \n",
       "16346  AI can analyze sensory preferences, recommend ...  \n",
       "11187  Segmentation divides a programs memory into l...  \n",
       "7427   \"Create an array or list, then use `for` follo...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "file_path = \"C:/Users/DELL/Desktop/CS QnA/final_dataset.csv\"  # Update the path if needed\n",
    "df = pd.read_csv(file_path)\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58661542-9b61-44bd-abe1-cf00414ef5fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20076, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70e921b5-5728-4685-aed2-9066fe2d8e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame to Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6895405-f4e2-4804-b549-f1977c2e0a86",
   "metadata": {},
   "source": [
    "# Checking Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e0446a1-3973-4f79-ac46-550e29e9d245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ab567dbc97b4e458d9bdc11cf509846",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20076 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question tokens: min=1, max=92, avg=11.61516238294481\n",
      "Answer tokens:   min=1, max=4293, avg=65.34085475194262\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "def add_token_length(example):\n",
    "    # Convert each row's question/answer to string, tokenize, and store lengths\n",
    "    example[\"question_tokens\"] = len(tokenizer.tokenize(str(example[\"question\"])))\n",
    "    example[\"answer_tokens\"] = len(tokenizer.tokenize(str(example[\"answer\"])))\n",
    "    return example\n",
    "\n",
    "# Use map() to add the new columns to your dataset\n",
    "dataset = dataset.map(add_token_length)\n",
    "\n",
    "# Now compute min and max over the entire dataset\n",
    "q_min = min(dataset[\"question_tokens\"])\n",
    "q_max = max(dataset[\"question_tokens\"])\n",
    "a_min = min(dataset[\"answer_tokens\"])\n",
    "a_max = max(dataset[\"answer_tokens\"])\n",
    "q_avg = statistics.mean(dataset[\"question_tokens\"])\n",
    "a_avg = statistics.mean(dataset[\"answer_tokens\"])\n",
    "\n",
    "\n",
    "print(f\"Question tokens: min={q_min}, max={q_max}, avg={q_avg}\")\n",
    "print(f\"Answer tokens:   min={a_min}, max={a_max}, avg={a_avg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a5bf3e-df6d-4b94-8bd6-695ae59f9f73",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "268355fd-306b-446c-aefc-769816a84cf0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38875aeb79164e91a3e4f1f6b19d697d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20076 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'What are the prerequisites to learn Microsoft Azure?', 'answer': 'To learn Microsoft Azure there is no need of any extra knowledge any one can easily learn the Azure if they have the below listed set of skills. Basic understanding of Azure concept.Understanding of Cloud ConceptsUnderstanding if basic infrastructure management, database management, and software development.', 'question_tokens': 10, 'answer_tokens': 52, 'question_input_ids': [0, 2264, 32, 5, 1198, 44877, 7, 1532, 3709, 25959, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'question_attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'answer_input_ids': [0, 3972, 1532, 3709, 25959, 89, 16, 117, 240, 9, 143, 1823, 2655, 143, 65, 64, 2773, 1532, 5, 25959, 114, 51, 33, 5, 874, 3147, 278, 9, 2417, 4, 17255, 2969, 9, 25959, 4286, 4, 44188, 9, 7941, 42939, 44188, 114, 3280, 2112, 1052, 6, 8503, 1052, 6, 8, 2257, 709, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'answer_attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "def tokenize_batch(batch):\n",
    "    # Tokenize questions in batch\n",
    "    tokenized_questions = tokenizer(\n",
    "        batch[\"question\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=100\n",
    "    )\n",
    "    \n",
    "    # Tokenize answers in batch; here truncation is disabled for answers\n",
    "    tokenized_answers = tokenizer(\n",
    "        batch[\"answer\"],\n",
    "        padding=\"max_length\",  # Padding will apply for sequences shorter than max_length\n",
    "        truncation=True,      # Answers will not be truncated even if longer than max_length\n",
    "        max_length=512         # Only used for padding; full sequence is kept if longer\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"question_input_ids\": tokenized_questions[\"input_ids\"],\n",
    "        \"question_attention_mask\": tokenized_questions[\"attention_mask\"],\n",
    "        \"answer_input_ids\": tokenized_answers[\"input_ids\"],\n",
    "        \"answer_attention_mask\": tokenized_answers[\"attention_mask\"],\n",
    "    }\n",
    "\n",
    "# Apply tokenization to the dataset in batches\n",
    "tokenized_dataset = dataset.map(tokenize_batch, batched=True)\n",
    "\n",
    "# Inspect the first example to verify tokenization\n",
    "print(tokenized_dataset[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abb1e12-4eb2-4fd7-9b69-8d6e63c6394c",
   "metadata": {},
   "source": [
    "# Train test validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "619bd6ca-0459-4be9-b563-feb7ce134606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 18050\n",
      "Validation size: 2006\n",
      "Test size: 20\n"
     ]
    }
   ],
   "source": [
    "# First, split out a fixed test set of 20 random examples.\n",
    "split_dataset = tokenized_dataset.train_test_split(test_size=20, seed=42)\n",
    "test_dataset = split_dataset[\"test\"]\n",
    "remaining_dataset = split_dataset[\"train\"]\n",
    "\n",
    "# Now, split the remaining dataset into 90% training and 10% validation.\n",
    "train_valid_split = remaining_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = train_valid_split[\"train\"]\n",
    "validation_dataset = train_valid_split[\"test\"]\n",
    "\n",
    "# Print out the sizes for verification:\n",
    "print(f\"Train size: {len(train_dataset)}\")\n",
    "print(f\"Validation size: {len(validation_dataset)}\")\n",
    "print(f\"Test size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460ba58d-e8be-411f-8e17-a68eb12bcfff",
   "metadata": {},
   "source": [
    "# Loading Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "968161f5-a021-4b94-bff5-fca0b0bd7b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93e7fa4f85844e9782d652707f984ade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/558M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\DELL\\.cache\\huggingface\\hub\\models--facebook--bart-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Load DistilBART model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-base\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8af87130-84df-4701-8593-e5d7b2c49bc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7735e69476c46fc8584c8dc3b94c8f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18050 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "173f98aeb1484d749a70a6c9fe71aeed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2006 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2121814aee394a37a3276f189d56ceef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def format_for_trainer(example):\n",
    "    return {\n",
    "        \"input_ids\": example[\"question_input_ids\"],\n",
    "        \"attention_mask\": example[\"question_attention_mask\"],\n",
    "        \"labels\": example[\"answer_input_ids\"],\n",
    "        \"question\": example[\"question\"],    # Keep original text for evaluation\n",
    "        \"answer\": example[\"answer\"]\n",
    "    }\n",
    "\n",
    "# Apply mapping without removing columns completely\n",
    "train_dataset = train_dataset.map(format_for_trainer)\n",
    "validation_dataset = validation_dataset.map(format_for_trainer)\n",
    "test_dataset = test_dataset.map(format_for_trainer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d7a564-25e0-4225-ad36-c01390e9672e",
   "metadata": {},
   "source": [
    "# Defining Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b714293-6f55-4754-affd-e5bb232fd18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data collator using your tokenizer and model\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "# Define a custom collate function to remove the raw text fields\n",
    "def custom_collate_fn(features):\n",
    "    # Remove the raw text keys (\"question\" and \"answer\")\n",
    "    cleaned_features = [\n",
    "        {k: v for k, v in feature.items() if k not in [\"question\", \"answer\"]}\n",
    "        for feature in features\n",
    "    ]\n",
    "    return data_collator(cleaned_features)\n",
    "\n",
    "# Define DataLoader with the custom collate_fn to convert lists to tensors\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=custom_collate_fn)\n",
    "val_loader = DataLoader(validation_dataset, batch_size=8, shuffle=False, collate_fn=custom_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, collate_fn=custom_collate_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bae138-1787-4f7a-a62a-32079d9546a1",
   "metadata": {},
   "source": [
    "# Evaluating model before training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49efe676-1c20-4d23-bad1-81ce7f9f32f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97608a8dc22945409fd6c7b29f817b6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Can you explain the concept of encapsulation in object-oriented programming?\n",
      "Reference Answer: Encapsulation is the concept of bundling data and functions (methods) together in a single unit known as a class in object-oriented programming. It allows for better organization and reusability of code by encapsulating data and functionality within a single entity.\n",
      "Generated Answer: Can you explain the concept of encapsulation in object-oriented programming?\n",
      "ROUGE Score: {'rouge1': 0.3076923076923077, 'rouge2': 0.2, 'rougeL': 0.2692307692307692, 'rougeLsum': 0.2692307692307692}\n",
      "--------------------------------------------------\n",
      "Question: What shape will the product matrix C have if matrix A is of shape m x n and matrix B is of shape n x p?\n",
      "Reference Answer: The product matrix C will be of shape m x p.\n",
      "Generated Answer: What shape will the product matrix C have if matrix A is of shape m x n and matrix B is ofshape n x p?\n",
      "ROUGE Score: {'rouge1': 0.5555555555555556, 'rouge2': 0.4117647058823529, 'rougeL': 0.5, 'rougeLsum': 0.5}\n",
      "--------------------------------------------------\n",
      "Question: How does the if-else-if ladder work in C++?\n",
      "Reference Answer: In C++, the if-else-if ladder helps decide among multiple options. The statements are executed from top down, and as soon as one condition is true, the associated block is executed, bypassing the rest.3 min read\n",
      "Generated Answer: How does the if-else-if ladder work in C++?\n",
      "ROUGE Score: {'rouge1': 0.29166666666666663, 'rouge2': 0.2173913043478261, 'rougeL': 0.20833333333333334, 'rougeLsum': 0.20833333333333334}\n",
      "--------------------------------------------------\n",
      "Question: What is Gradle?\n",
      "Reference Answer: Gradle is an excellent open-source construction tool for software development, developed by Hans Dockter and others.\n",
      "Generated Answer: What is Gradle?\n",
      "ROUGE Score: {'rouge1': 0.2, 'rouge2': 0.0, 'rougeL': 0.1, 'rougeLsum': 0.1}\n",
      "--------------------------------------------------\n",
      "Question: How to use PHP echo and print?\n",
      "Reference Answer: PHP echo and print are two most language constructs used for output data on the screen. They are not functions but constructs, meaning they do not require parentheses (though parentheses can be used with print). Both are widely used for displaying strings, variables, and HTML content in PHP scripts.\n",
      "Generated Answer: How to use PHP echo and print?\n",
      "ROUGE Score: {'rouge1': 0.14285714285714285, 'rouge2': 0.1111111111111111, 'rougeL': 0.14285714285714285, 'rougeLsum': 0.14285714285714285}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# For reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Load the ROUGE metric using the Hugging Face evaluate library.\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "# Randomly select 5 examples from your test dataset.\n",
    "sample_indices = random.sample(range(len(test_dataset)), 5)\n",
    "\n",
    "for idx in sample_indices:\n",
    "    example = test_dataset[idx]\n",
    "    \n",
    "    # Extract the question and reference answer\n",
    "    question_text = example[\"question\"]\n",
    "    reference_answer = example[\"answer\"]\n",
    "    \n",
    "    print(\"Question:\", question_text)\n",
    "    print(\"Reference Answer:\", reference_answer)\n",
    "    \n",
    "    # Tokenize the question with a max length of 100 tokens\n",
    "    inputs = tokenizer(\n",
    "        question_text,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=100  # Input question length set to 100 tokens\n",
    "    ).to(device)\n",
    "    \n",
    "    # Generate an answer with a maximum of 512 tokens\n",
    "    generated_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_length=512,   # Generated answer can be up to 512 tokens\n",
    "        num_beams=5,      # Using beam search for improved output quality\n",
    "        early_stopping=True\n",
    "    )\n",
    "    \n",
    "    # Decode the generated tokens into text\n",
    "    generated_answer = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    print(\"Generated Answer:\", generated_answer)\n",
    "    \n",
    "    # Compute the ROUGE score between the generated answer and the reference answer.\n",
    "    result = rouge.compute(predictions=[generated_answer], references=[reference_answer])\n",
    "    print(\"ROUGE Score:\", result)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad9d145-bd46-4861-b394-0f27fd34a85c",
   "metadata": {},
   "source": [
    "# Training the Model to determine Epoch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9b3105d6-15be-4992-b8c3-4a07015a4b15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m     latest_checkpoint \u001b[38;5;241m=\u001b[39m checkpoint_files[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# Get the most recent checkpoint\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     latest_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(latest_checkpoint\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_epoch_\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m])  \u001b[38;5;66;03m# Extract the epoch number\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(latest_checkpoint, weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded checkpoint from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlatest_checkpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Resuming training from epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlatest_epoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:1351\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[0;32m   1350\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1351\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[0;32m   1352\u001b[0m             opened_zipfile,\n\u001b[0;32m   1353\u001b[0m             map_location,\n\u001b[0;32m   1354\u001b[0m             _weights_only_unpickler,\n\u001b[0;32m   1355\u001b[0m             overall_storage\u001b[38;5;241m=\u001b[39moverall_storage,\n\u001b[0;32m   1356\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[0;32m   1357\u001b[0m         )\n\u001b[0;32m   1358\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1359\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:1848\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m _serialization_tls\n\u001b[0;32m   1847\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m map_location\n\u001b[1;32m-> 1848\u001b[0m result \u001b[38;5;241m=\u001b[39m unpickler\u001b[38;5;241m.\u001b[39mload()\n\u001b[0;32m   1849\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1851\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\_weights_only_unpickler.py:385\u001b[0m, in \u001b[0;36mUnpickler.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    378\u001b[0m         \u001b[38;5;28mtype\u001b[39m(pid) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m\n\u001b[0;32m    379\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pid) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    380\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mserialization\u001b[38;5;241m.\u001b[39m_maybe_decode_ascii(pid[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    381\u001b[0m     ):\n\u001b[0;32m    382\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m UnpicklingError(\n\u001b[0;32m    383\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly persistent_load of storage is allowed, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpid[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    384\u001b[0m         )\n\u001b[1;32m--> 385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpersistent_load(pid))\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [BINGET[\u001b[38;5;241m0\u001b[39m], LONG_BINGET[\u001b[38;5;241m0\u001b[39m]]:\n\u001b[0;32m    387\u001b[0m     idx \u001b[38;5;241m=\u001b[39m (read(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m key[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m BINGET[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m unpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<I\u001b[39m\u001b[38;5;124m\"\u001b[39m, read(\u001b[38;5;241m4\u001b[39m)))[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:1812\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[1;34m(saved_id)\u001b[0m\n\u001b[0;32m   1810\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1811\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[1;32m-> 1812\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m load_tensor(\n\u001b[0;32m   1813\u001b[0m         dtype, nbytes, key, _maybe_decode_ascii(location)\n\u001b[0;32m   1814\u001b[0m     )\n\u001b[0;32m   1816\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:1784\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[1;34m(dtype, numel, key, location)\u001b[0m\n\u001b[0;32m   1779\u001b[0m         storage\u001b[38;5;241m.\u001b[39mbyteswap(dtype)\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[1;32m-> 1784\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39mrestore_location(storage, location),\n\u001b[0;32m   1785\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m   1786\u001b[0m     _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   1787\u001b[0m )\n\u001b[0;32m   1789\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typed_storage\u001b[38;5;241m.\u001b[39m_data_ptr() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1790\u001b[0m     loaded_storages[key] \u001b[38;5;241m=\u001b[39m typed_storage\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:601\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[1;34m(storage, location)\u001b[0m\n\u001b[0;32m    581\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    582\u001b[0m \u001b[38;5;124;03mRestores `storage` using a deserializer function registered for the `location`.\u001b[39;00m\n\u001b[0;32m    583\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;124;03m       all matching ones return `None`.\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    600\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[1;32m--> 601\u001b[0m     result \u001b[38;5;241m=\u001b[39m fn(storage, location)\n\u001b[0;32m    602\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    603\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:539\u001b[0m, in \u001b[0;36m_deserialize\u001b[1;34m(backend_name, obj, location)\u001b[0m\n\u001b[0;32m    537\u001b[0m     backend_name \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_privateuse1_backend_name()\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m location\u001b[38;5;241m.\u001b[39mstartswith(backend_name):\n\u001b[1;32m--> 539\u001b[0m     device \u001b[38;5;241m=\u001b[39m _validate_device(location, backend_name)\n\u001b[0;32m    540\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:508\u001b[0m, in \u001b[0;36m_validate_device\u001b[1;34m(location, backend_name)\u001b[0m\n\u001b[0;32m    506\u001b[0m     device_index \u001b[38;5;241m=\u001b[39m device\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;28;01mif\u001b[39;00m device\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(device_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_available\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m device_module\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m--> 508\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    509\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to deserialize object on a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend_name\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    510\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice but torch.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.is_available() is False. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    511\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you are running on a CPU-only machine, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    512\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    513\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto map your storages to the CPU.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    514\u001b[0m     )\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(device_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice_count\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    516\u001b[0m     device_count \u001b[38;5;241m=\u001b[39m device_module\u001b[38;5;241m.\u001b[39mdevice_count()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "import glob  # Used to find checkpoint files\n",
    "\n",
    "# Set training parameters\n",
    "num_epochs = 30\n",
    "learning_rate = 5e-5\n",
    "\n",
    "# Create the directory to store checkpoints and metrics if it doesn't exist\n",
    "checkpoint_dir = \"CS QnA\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "best_model_path = os.path.join(checkpoint_dir, \"best_model.pt\")\n",
    "metrics_path = os.path.join(checkpoint_dir, \"best_metrics.json\")\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Load the ROUGE metric (we'll use ROUGE-L as our key metric here)\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "# ----- Find the latest checkpoint -----\n",
    "checkpoint_files = glob.glob(os.path.join(checkpoint_dir, \"checkpoint_epoch_*.pt\"))\n",
    "latest_epoch = 0  # Start from the first epoch if no checkpoint exists\n",
    "\n",
    "if checkpoint_files:\n",
    "    # Sort checkpoints by epoch number\n",
    "    checkpoint_files.sort(key=lambda x: int(x.split(\"_epoch_\")[-1].split(\".pt\")[0]))\n",
    "    latest_checkpoint = checkpoint_files[-1]  # Get the most recent checkpoint\n",
    "    latest_epoch = int(latest_checkpoint.split(\"_epoch_\")[-1].split(\".pt\")[0])  # Extract the epoch number\n",
    "    model.load_state_dict(torch.load(latest_checkpoint, weights_only=True))\n",
    "    print(f\"Loaded checkpoint from {latest_checkpoint}. Resuming training from epoch {latest_epoch + 1}.\")\n",
    "else:\n",
    "    print(\"No previous checkpoints found. Starting training from scratch.\")\n",
    "\n",
    "# --- Load best metrics if they exist ---\n",
    "if os.path.exists(metrics_path):\n",
    "    with open(metrics_path, \"r\") as f:\n",
    "        best_metrics = json.load(f)\n",
    "    best_rouge = best_metrics.get(\"best_rouge\", 0.0)\n",
    "    best_epoch = best_metrics.get(\"best_epoch\", 0)\n",
    "    print(f\"Loaded best metrics: best_rouge = {best_rouge}, best_epoch = {best_epoch}\")\n",
    "else:\n",
    "    best_rouge = 0.0\n",
    "    best_epoch = 0\n",
    "\n",
    "# ----- Start Training Loop -----\n",
    "for epoch in range(latest_epoch + 1, num_epochs + 1):  # Start from next epoch\n",
    "    print(f\"\\n===== Epoch {epoch}/{num_epochs} =====\")\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "\n",
    "    # ----- Training Phase -----\n",
    "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Move batch tensors to GPU\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Forward pass (model returns loss because we supply labels)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    print(f\"Average Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # ----- Save Checkpoint for This Epoch -----\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_epoch_{epoch}.pt\")\n",
    "    torch.save(model.state_dict(), checkpoint_path)\n",
    "    print(f\"Checkpoint saved at: {checkpoint_path}\")\n",
    "\n",
    "    # ----- Evaluation Phase -----\n",
    "    model.eval()\n",
    "    rouge_scores = []\n",
    "\n",
    "    # Run validation on all samples in the validation dataset\n",
    "    for sample in validation_dataset:\n",
    "        # Assuming the original text is preserved in these keys\n",
    "        question_text = sample[\"question\"]\n",
    "        reference_answer = sample[\"answer\"]\n",
    "\n",
    "        # Tokenize question (max 100 tokens)\n",
    "        inputs = tokenizer(\n",
    "            question_text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=100\n",
    "        )\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "        # Generate answer with max_length=512, beam search for better quality\n",
    "        generated_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_length=512,\n",
    "            num_beams=5,\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "        generated_answer = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        # Compute ROUGE score for this sample (using ROUGE-L)\n",
    "        result = rouge_metric.compute(predictions=[generated_answer], references=[reference_answer])\n",
    "        sample_rouge = result[\"rougeL\"]  # You can choose another metric or average multiple\n",
    "        rouge_scores.append(sample_rouge)\n",
    "\n",
    "    avg_rouge = np.mean(rouge_scores)\n",
    "    print(f\"Epoch {epoch} Average ROUGE-L: {avg_rouge:.4f}\")\n",
    "\n",
    "    # ----- Log the ROUGE Score -----\n",
    "    with open(os.path.join(checkpoint_dir, \"rouge_log.txt\"), \"a\") as f:\n",
    "        f.write(f\"Epoch {epoch}: Average ROUGE-L = {avg_rouge:.4f}\\n\")\n",
    "\n",
    "    # ----- Check for Improvement -----\n",
    "    if avg_rouge > best_rouge:\n",
    "        best_rouge = avg_rouge\n",
    "        best_epoch = epoch\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(\"New best model found and saved!\")\n",
    "\n",
    "    # Save the updated best metrics so they can be loaded next time  \n",
    "    with open(metrics_path, \"w\") as f:\n",
    "        json.dump({\"best_rouge\": best_rouge, \"best_epoch\": best_epoch}, f)\n",
    "\n",
    "print(f\"\\nTraining complete. Best model from epoch {best_epoch} with ROUGE-L: {best_rouge:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63127232-35a5-4359-a1b3-16c46d2f0aef",
   "metadata": {},
   "source": [
    "# Evaluating Random 5 Test Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cd283f8e-200b-4a97-9523-dcf57ea958d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Check if best_model.pt exists\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(best_model_path):\n\u001b[1;32m----> 7\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(best_model_path, weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))  \u001b[38;5;66;03m# Load the best-performing model\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Loaded best model from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_model_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:1351\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[0;32m   1350\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1351\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[0;32m   1352\u001b[0m             opened_zipfile,\n\u001b[0;32m   1353\u001b[0m             map_location,\n\u001b[0;32m   1354\u001b[0m             _weights_only_unpickler,\n\u001b[0;32m   1355\u001b[0m             overall_storage\u001b[38;5;241m=\u001b[39moverall_storage,\n\u001b[0;32m   1356\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[0;32m   1357\u001b[0m         )\n\u001b[0;32m   1358\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1359\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:1848\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m _serialization_tls\n\u001b[0;32m   1847\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m map_location\n\u001b[1;32m-> 1848\u001b[0m result \u001b[38;5;241m=\u001b[39m unpickler\u001b[38;5;241m.\u001b[39mload()\n\u001b[0;32m   1849\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1851\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\_weights_only_unpickler.py:385\u001b[0m, in \u001b[0;36mUnpickler.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    378\u001b[0m         \u001b[38;5;28mtype\u001b[39m(pid) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m\n\u001b[0;32m    379\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pid) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    380\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mserialization\u001b[38;5;241m.\u001b[39m_maybe_decode_ascii(pid[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    381\u001b[0m     ):\n\u001b[0;32m    382\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m UnpicklingError(\n\u001b[0;32m    383\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly persistent_load of storage is allowed, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpid[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    384\u001b[0m         )\n\u001b[1;32m--> 385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpersistent_load(pid))\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [BINGET[\u001b[38;5;241m0\u001b[39m], LONG_BINGET[\u001b[38;5;241m0\u001b[39m]]:\n\u001b[0;32m    387\u001b[0m     idx \u001b[38;5;241m=\u001b[39m (read(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m key[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m BINGET[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m unpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<I\u001b[39m\u001b[38;5;124m\"\u001b[39m, read(\u001b[38;5;241m4\u001b[39m)))[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:1812\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[1;34m(saved_id)\u001b[0m\n\u001b[0;32m   1810\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1811\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[1;32m-> 1812\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m load_tensor(\n\u001b[0;32m   1813\u001b[0m         dtype, nbytes, key, _maybe_decode_ascii(location)\n\u001b[0;32m   1814\u001b[0m     )\n\u001b[0;32m   1816\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:1784\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[1;34m(dtype, numel, key, location)\u001b[0m\n\u001b[0;32m   1779\u001b[0m         storage\u001b[38;5;241m.\u001b[39mbyteswap(dtype)\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[1;32m-> 1784\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39mrestore_location(storage, location),\n\u001b[0;32m   1785\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m   1786\u001b[0m     _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   1787\u001b[0m )\n\u001b[0;32m   1789\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typed_storage\u001b[38;5;241m.\u001b[39m_data_ptr() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1790\u001b[0m     loaded_storages[key] \u001b[38;5;241m=\u001b[39m typed_storage\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:601\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[1;34m(storage, location)\u001b[0m\n\u001b[0;32m    581\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    582\u001b[0m \u001b[38;5;124;03mRestores `storage` using a deserializer function registered for the `location`.\u001b[39;00m\n\u001b[0;32m    583\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;124;03m       all matching ones return `None`.\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    600\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[1;32m--> 601\u001b[0m     result \u001b[38;5;241m=\u001b[39m fn(storage, location)\n\u001b[0;32m    602\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    603\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:539\u001b[0m, in \u001b[0;36m_deserialize\u001b[1;34m(backend_name, obj, location)\u001b[0m\n\u001b[0;32m    537\u001b[0m     backend_name \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_privateuse1_backend_name()\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m location\u001b[38;5;241m.\u001b[39mstartswith(backend_name):\n\u001b[1;32m--> 539\u001b[0m     device \u001b[38;5;241m=\u001b[39m _validate_device(location, backend_name)\n\u001b[0;32m    540\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:508\u001b[0m, in \u001b[0;36m_validate_device\u001b[1;34m(location, backend_name)\u001b[0m\n\u001b[0;32m    506\u001b[0m     device_index \u001b[38;5;241m=\u001b[39m device\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;28;01mif\u001b[39;00m device\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(device_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_available\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m device_module\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m--> 508\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    509\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to deserialize object on a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend_name\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    510\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice but torch.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.is_available() is False. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    511\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you are running on a CPU-only machine, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    512\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    513\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto map your storages to the CPU.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    514\u001b[0m     )\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(device_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice_count\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    516\u001b[0m     device_count \u001b[38;5;241m=\u001b[39m device_module\u001b[38;5;241m.\u001b[39mdevice_count()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "# Set checkpoint directory\n",
    "checkpoint_dir = \"CS QnA\"\n",
    "best_model_path = os.path.join(checkpoint_dir, \"best_model.pt\")\n",
    "\n",
    "# Check if best_model.pt exists\n",
    "if os.path.exists(best_model_path):\n",
    "    model.load_state_dict(torch.load(best_model_path, weights_only=True))  # Load the best-performing model\n",
    "    print(f\" Loaded best model from {best_model_path}\")\n",
    "else:\n",
    "    print(\" No best model found! Make sure training has completed and best_model.pt exists.\")\n",
    "    exit()\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Load ROUGE metric\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "# Randomly select 5 examples from test dataset\n",
    "random.seed(42)  # Ensure reproducibility\n",
    "sample_indices = random.sample(range(len(test_dataset)), 5)\n",
    "\n",
    "for idx in sample_indices:\n",
    "    example = test_dataset[idx]\n",
    "    \n",
    "    # Extract question and reference answer\n",
    "    question_text = example[\"question\"]\n",
    "    reference_answer = example[\"answer\"]\n",
    "    \n",
    "    print(\"\\n **Question:**\", question_text)\n",
    "    print(\" **Reference Answer:**\", reference_answer)\n",
    "    \n",
    "    # Tokenize question\n",
    "    inputs = tokenizer(\n",
    "        question_text,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=100\n",
    "    ).to(device)\n",
    "\n",
    "    # Generate an answer from the best model\n",
    "    generated_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_length=512,\n",
    "        num_beams=5,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    # Decode generated answer\n",
    "    generated_answer = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(\" **Generated Answer:**\", generated_answer)\n",
    "    \n",
    "    # Compute ROUGE score\n",
    "    result = rouge.compute(predictions=[generated_answer], references=[reference_answer])\n",
    "    print(\" **ROUGE Score:**\", result)\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a86b10-ccb0-49d7-8f30-8797016f74b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set checkpoint directory\n",
    "checkpoint_dir = \"CS QnA\"\n",
    "best_model_path = os.path.join(checkpoint_dir, \"best_model.pt\")\n",
    "\n",
    "# Check if best_model.pt exists\n",
    "if os.path.exists(best_model_path):\n",
    "    model.load_state_dict(torch.load(best_model_path, weights_only=True))  # Load the best-performing model\n",
    "    print(f\" Loaded best model from {best_model_path}\")\n",
    "else:\n",
    "    print(\" No best model found! Make sure training has completed and best_model.pt exists.\")\n",
    "    exit()\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Load ROUGE metric\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "# Initialize lists for storing predictions and references\n",
    "all_predictions = []\n",
    "all_references = []\n",
    "\n",
    "# Loop over every example in the test dataset\n",
    "for example in test_dataset:\n",
    "    # Extract question and reference answer\n",
    "    question_text = example[\"question\"]\n",
    "    reference_answer = example[\"answer\"]\n",
    "    \n",
    "    # Tokenize question\n",
    "    inputs = tokenizer(\n",
    "        question_text,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=100\n",
    "    ).to(device)\n",
    "\n",
    "    # Generate an answer using the model\n",
    "    generated_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_length=512,\n",
    "        num_beams=5,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    # Decode the generated answer\n",
    "    generated_answer = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Append the generated answer and reference to their respective lists\n",
    "    all_predictions.append(generated_answer)\n",
    "    all_references.append(reference_answer)\n",
    "\n",
    "# Compute the average ROUGE score for the entire test dataset\n",
    "avg_result = rouge.compute(predictions=all_predictions, references=all_references)\n",
    "print(\" **Average ROUGE Score:**\", avg_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a9028a-63ac-4b37-a216-d346376816bd",
   "metadata": {},
   "source": [
    "# Self-testing for Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7e2a5b0c-d928-494a-9fad-c916a92431de",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Check if best_model.pt exists and load it\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(best_model_path):\n\u001b[1;32m---> 10\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(best_model_path, weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))  \u001b[38;5;66;03m# Load best-performing model\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Loaded best model from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_model_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:1351\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[0;32m   1350\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1351\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[0;32m   1352\u001b[0m             opened_zipfile,\n\u001b[0;32m   1353\u001b[0m             map_location,\n\u001b[0;32m   1354\u001b[0m             _weights_only_unpickler,\n\u001b[0;32m   1355\u001b[0m             overall_storage\u001b[38;5;241m=\u001b[39moverall_storage,\n\u001b[0;32m   1356\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[0;32m   1357\u001b[0m         )\n\u001b[0;32m   1358\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1359\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:1848\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m _serialization_tls\n\u001b[0;32m   1847\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m map_location\n\u001b[1;32m-> 1848\u001b[0m result \u001b[38;5;241m=\u001b[39m unpickler\u001b[38;5;241m.\u001b[39mload()\n\u001b[0;32m   1849\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1851\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\_weights_only_unpickler.py:385\u001b[0m, in \u001b[0;36mUnpickler.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    378\u001b[0m         \u001b[38;5;28mtype\u001b[39m(pid) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m\n\u001b[0;32m    379\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pid) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    380\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mserialization\u001b[38;5;241m.\u001b[39m_maybe_decode_ascii(pid[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    381\u001b[0m     ):\n\u001b[0;32m    382\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m UnpicklingError(\n\u001b[0;32m    383\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly persistent_load of storage is allowed, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpid[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    384\u001b[0m         )\n\u001b[1;32m--> 385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpersistent_load(pid))\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [BINGET[\u001b[38;5;241m0\u001b[39m], LONG_BINGET[\u001b[38;5;241m0\u001b[39m]]:\n\u001b[0;32m    387\u001b[0m     idx \u001b[38;5;241m=\u001b[39m (read(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m key[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m BINGET[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m unpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<I\u001b[39m\u001b[38;5;124m\"\u001b[39m, read(\u001b[38;5;241m4\u001b[39m)))[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:1812\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[1;34m(saved_id)\u001b[0m\n\u001b[0;32m   1810\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1811\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[1;32m-> 1812\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m load_tensor(\n\u001b[0;32m   1813\u001b[0m         dtype, nbytes, key, _maybe_decode_ascii(location)\n\u001b[0;32m   1814\u001b[0m     )\n\u001b[0;32m   1816\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:1784\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[1;34m(dtype, numel, key, location)\u001b[0m\n\u001b[0;32m   1779\u001b[0m         storage\u001b[38;5;241m.\u001b[39mbyteswap(dtype)\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[1;32m-> 1784\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39mrestore_location(storage, location),\n\u001b[0;32m   1785\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m   1786\u001b[0m     _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   1787\u001b[0m )\n\u001b[0;32m   1789\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typed_storage\u001b[38;5;241m.\u001b[39m_data_ptr() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1790\u001b[0m     loaded_storages[key] \u001b[38;5;241m=\u001b[39m typed_storage\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:601\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[1;34m(storage, location)\u001b[0m\n\u001b[0;32m    581\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    582\u001b[0m \u001b[38;5;124;03mRestores `storage` using a deserializer function registered for the `location`.\u001b[39;00m\n\u001b[0;32m    583\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;124;03m       all matching ones return `None`.\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    600\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[1;32m--> 601\u001b[0m     result \u001b[38;5;241m=\u001b[39m fn(storage, location)\n\u001b[0;32m    602\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    603\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:539\u001b[0m, in \u001b[0;36m_deserialize\u001b[1;34m(backend_name, obj, location)\u001b[0m\n\u001b[0;32m    537\u001b[0m     backend_name \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_privateuse1_backend_name()\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m location\u001b[38;5;241m.\u001b[39mstartswith(backend_name):\n\u001b[1;32m--> 539\u001b[0m     device \u001b[38;5;241m=\u001b[39m _validate_device(location, backend_name)\n\u001b[0;32m    540\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:508\u001b[0m, in \u001b[0;36m_validate_device\u001b[1;34m(location, backend_name)\u001b[0m\n\u001b[0;32m    506\u001b[0m     device_index \u001b[38;5;241m=\u001b[39m device\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;28;01mif\u001b[39;00m device\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(device_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_available\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m device_module\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m--> 508\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    509\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to deserialize object on a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend_name\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    510\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice but torch.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.is_available() is False. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    511\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you are running on a CPU-only machine, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    512\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    513\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto map your storages to the CPU.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    514\u001b[0m     )\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(device_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice_count\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    516\u001b[0m     device_count \u001b[38;5;241m=\u001b[39m device_module\u001b[38;5;241m.\u001b[39mdevice_count()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# Set checkpoint directory and best model path\n",
    "checkpoint_dir = \"CS QnA\"\n",
    "best_model_path = os.path.join(checkpoint_dir, \"best_model.pt\")\n",
    "\n",
    "# Check if best_model.pt exists and load it\n",
    "if os.path.exists(best_model_path):\n",
    "    model.load_state_dict(torch.load(best_model_path, weights_only=True))  # Load best-performing model\n",
    "    print(f\" Loaded best model from {best_model_path}\")\n",
    "else:\n",
    "    print(\" No best model found! Make sure training has completed and best_model.pt exists.\")\n",
    "    exit()\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Interactive question input loop\n",
    "while True:\n",
    "    user_question = input(\"\\n Enter your question (or type 'exit' to quit): \").strip()\n",
    "    \n",
    "    if user_question.lower() == \"exit\":\n",
    "        print(\" Exiting...\")\n",
    "        break\n",
    "    \n",
    "    # Tokenize the input question\n",
    "    inputs = tokenizer(\n",
    "        user_question,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=100\n",
    "    ).to(device)\n",
    "\n",
    "    # Generate an answer from the model\n",
    "    generated_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_length=512,\n",
    "        num_beams=5,  # Using beam search for better quality\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    # Decode the generated answer\n",
    "    generated_answer = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(\"\\n **Generated Answer:**\", generated_answer)\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7304c4-f76c-4d4b-bd39-b061cdca8e64",
   "metadata": {},
   "source": [
    "# Merging train and validation dataset for final training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04af0619-5305-4bd6-9511-d69b6a6a354a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "# Merge train and validation datasets\n",
    "merged_train_dataset = concatenate_datasets([train_dataset, validation_dataset])\n",
    "\n",
    "# Print out the new sizes for verification\n",
    "print(f\"Merged Train size: {len(merged_train_dataset)}\")\n",
    "print(f\"Test size (unchanged): {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0da4f54-46b4-4485-8dee-0890eb992db8",
   "metadata": {},
   "source": [
    "# Dataloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02db64f1-07a0-45ce-b68d-98b7d6b6a922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data collator using your tokenizer and model\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "# Define a custom collate function to remove the raw text fields (\"question\" and \"answer\")\n",
    "def custom_collate_fn(features):\n",
    "    cleaned_features = [\n",
    "        {k: v for k, v in feature.items() if k not in [\"question\", \"answer\"]}\n",
    "        for feature in features\n",
    "    ]\n",
    "    return data_collator(cleaned_features)\n",
    "\n",
    "# Create DataLoaders using the merged training dataset and the test dataset\n",
    "train_loader = DataLoader(merged_train_dataset, batch_size=8, shuffle=True, collate_fn=custom_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, collate_fn=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98df405-c028-4076-8de4-d3b742f725c7",
   "metadata": {},
   "source": [
    "# Final Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b941f65-69a1-4a71-bc99-df93166e3664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training parameters\n",
    "num_epochs = 21\n",
    "learning_rate = 5e-5\n",
    "\n",
    "# Create the directory to store checkpoints and the final model\n",
    "checkpoint_dir = \"final_model\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "final_model_name = \"final_trained_model\"\n",
    "\n",
    "# Create or open a log file for training loss\n",
    "loss_log_path = os.path.join(checkpoint_dir, \"training_loss_log.txt\")\n",
    "with open(loss_log_path, \"a\") as log_file:\n",
    "    log_file.write(\"Training Loss Log:\\n\")\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# ----- Find the latest checkpoint if available -----\n",
    "checkpoint_files = glob.glob(os.path.join(checkpoint_dir, f\"{final_model_name}_epoch_*.pt\"))\n",
    "latest_epoch = 0  # Start from scratch if no checkpoint exists\n",
    "\n",
    "if checkpoint_files:\n",
    "    # Sort checkpoints by epoch number\n",
    "    checkpoint_files.sort(key=lambda x: int(x.split(\"_epoch_\")[-1].split(\".pt\")[0]))\n",
    "    latest_checkpoint = checkpoint_files[-1]\n",
    "    latest_epoch = int(latest_checkpoint.split(\"_epoch_\")[-1].split(\".pt\")[0])\n",
    "    model.load_state_dict(torch.load(latest_checkpoint))\n",
    "    print(f\"Loaded checkpoint from {latest_checkpoint}. Resuming training from epoch {latest_epoch + 1}.\")\n",
    "else:\n",
    "    print(\"No previous checkpoints found. Starting training from scratch.\")\n",
    "\n",
    "# ----- Start Training Loop -----\n",
    "for epoch in range(latest_epoch + 1, num_epochs + 1):  # Start from the next epoch\n",
    "    print(f\"\\n===== Epoch {epoch}/{num_epochs} =====\")\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "\n",
    "    # ----- Training Phase -----\n",
    "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Move batch tensors to GPU\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Forward pass (model returns loss because we supply labels)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    print(f\"Average Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Log the training loss for this epoch in human-readable format\n",
    "    with open(loss_log_path, \"a\") as log_file:\n",
    "        log_file.write(f\"Epoch {epoch}: Training loss = {avg_train_loss:.4f}\\n\")\n",
    "\n",
    "    # ----- Save Checkpoint for This Epoch -----\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f\"{final_model_name}_epoch_{epoch}.pt\")\n",
    "    torch.save(model.state_dict(), checkpoint_path)\n",
    "    print(f\"Checkpoint saved at: {checkpoint_path}\")\n",
    "\n",
    "print(f\"\\nTraining complete after {num_epochs} epochs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a653266-6337-4c18-876a-7cf3f5b90293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "\n",
    "# Set checkpoint directory and final model name (should match training code)\n",
    "checkpoint_dir = \"final_model\"\n",
    "final_model_name = \"final_trained_model\"\n",
    "\n",
    "# Construct checkpoint file pattern and search for checkpoints\n",
    "checkpoint_pattern = os.path.join(checkpoint_dir, f\"{final_model_name}_epoch_*.pt\")\n",
    "checkpoint_files = glob.glob(checkpoint_pattern)\n",
    "\n",
    "if checkpoint_files:\n",
    "    # Sort checkpoints by epoch number and select the latest checkpoint\n",
    "    checkpoint_files.sort(key=lambda x: int(x.split(\"_epoch_\")[-1].split(\".pt\")[0]))\n",
    "    latest_checkpoint = checkpoint_files[-1]\n",
    "    model.load_state_dict(torch.load(latest_checkpoint))\n",
    "    print(f\" Loaded model from checkpoint: {latest_checkpoint}\")\n",
    "else:\n",
    "    print(\" No checkpoints found. Please ensure training has been completed.\")\n",
    "    exit()\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Interactive question input loop\n",
    "while True:\n",
    "    user_question = input(\"\\n Enter your question (or type 'exit' to quit): \").strip()\n",
    "    \n",
    "    if user_question.lower() == \"exit\":\n",
    "        print(\" Exiting...\")\n",
    "        break\n",
    "    \n",
    "    # Tokenize the input question\n",
    "    inputs = tokenizer(\n",
    "        user_question,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=100\n",
    "    ).to(device)\n",
    "\n",
    "    # Generate an answer from the model\n",
    "    generated_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_length=512,\n",
    "        num_beams=5,      # Using beam search for better quality\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    # Decode the generated answer\n",
    "    generated_answer = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(\"\\n **Generated Answer:**\", generated_answer)\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba68c5a9-21f1-4fc9-9753-548f48fcecc6",
   "metadata": {},
   "source": [
    "# Final Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23ca9e5-efa1-434c-84a2-d7494679a196",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import evaluate\n",
    "\n",
    "############################################\n",
    "# 1. Load your fine-tuned model & tokenizer\n",
    "############################################\n",
    "\n",
    "def load_model_and_tokenizer():\n",
    "    model_name = \"facebook/bart-base\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Locate the latest checkpoint in your \"final_model\" folder\n",
    "    checkpoint_dir = \"final_model\"\n",
    "    model_prefix = \"final_trained_model\"\n",
    "    checkpoint_pattern = os.path.join(checkpoint_dir, f\"{model_prefix}_epoch_*.pt\")\n",
    "    checkpoint_files = glob.glob(checkpoint_pattern)\n",
    "\n",
    "    if checkpoint_files:\n",
    "        checkpoint_files.sort(key=lambda x: int(x.split(\"_epoch_\")[-1].split(\".pt\")[0]))\n",
    "        best_model_path = checkpoint_files[-1]\n",
    "        model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "        print(f\"Loaded fine-tuned model from {best_model_path}\")\n",
    "    else:\n",
    "        raise ValueError(\"No trained model checkpoint found in 'final_model' directory!\")\n",
    "\n",
    "    model.eval()\n",
    "    return model, tokenizer, device\n",
    "\n",
    "model, tokenizer, device = load_model_and_tokenizer()\n",
    "\n",
    "############################################\n",
    "# 2. Define a function to generate answers\n",
    "############################################\n",
    "\n",
    "def generate_answer(question: str) -> str:\n",
    "    inputs = tokenizer(\n",
    "        question,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=100\n",
    "    ).to(device)\n",
    "\n",
    "    # Generate answer with min_length increased to 200 and max_length set to 512 tokens\n",
    "    generated_ids = model.generate(\n",
    "        **inputs,\n",
    "        min_length=200,\n",
    "        max_length=512,\n",
    "        num_beams=5,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    return tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "############################################\n",
    "# 3. Create 20 comprehensive computer science Q&A pairs\n",
    "#    (using triple-quoted strings for the reference answers)\n",
    "############################################\n",
    "\n",
    "sample_data = [\n",
    "    {\n",
    "        \"question\": \"What is a computer?\",\n",
    "        \"answer\": \"\"\"A computer is an electronic device that processes data and performs calculations according to a set of instructions.\n",
    "It consists of hardware components, such as the CPU, memory, and storage, and software that runs applications and manages system resources.\n",
    "Computers are used in various fields to perform tasks ranging from simple calculations to complex simulations.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is hardware?\",\n",
    "        \"answer\": \"\"\"Hardware refers to the physical components of a computer system.\n",
    "This includes devices like the processor, memory modules, storage drives, and input/output peripherals.\n",
    "These tangible parts work together to enable the computer to function efficiently.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is software?\",\n",
    "        \"answer\": \"\"\"Software is a collection of programs, procedures, and routines that instruct a computer on how to perform tasks.\n",
    "It includes operating systems, applications, and utilities.\n",
    "Software development involves coding, testing, and maintaining these programs.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is an operating system?\",\n",
    "        \"answer\": \"\"\"An operating system (OS) is system software that manages computer hardware, software resources, and provides common services for computer programs.\n",
    "It acts as an intermediary between applications and computer hardware.\n",
    "Examples include Windows, macOS, and Linux.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is programming?\",\n",
    "        \"answer\": \"\"\"Programming is the process of designing, writing, testing, and maintaining code that instructs a computer to perform specific tasks.\n",
    "It involves using programming languages to develop software applications and solve problems systematically.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is software engineering?\",\n",
    "        \"answer\": \"\"\"Software engineering is the disciplined application of engineering principles to design, develop, test, and maintain software.\n",
    "It focuses on creating reliable, scalable, and efficient software solutions through systematic processes and methodologies.\n",
    "Software engineers work on everything from application development to system architecture.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is data science?\",\n",
    "        \"answer\": \"\"\"Data science is an interdisciplinary field that uses scientific methods, processes, and algorithms to extract knowledge and insights from data.\n",
    "It combines techniques from statistics, computer science, and domain expertise to analyze complex datasets.\n",
    "Data science is widely applied in business, research, and technology.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is machine learning?\",\n",
    "        \"answer\": \"\"\"Machine learning is a subset of artificial intelligence that enables systems to learn from data and improve over time without explicit programming.\n",
    "It uses statistical techniques to identify patterns and make predictions or decisions.\n",
    "Applications include image recognition, NLP, and recommendation systems.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is natural language processing (NLP)?\",\n",
    "        \"answer\": \"\"\"Natural language processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and human language.\n",
    "It involves enabling computers to understand, interpret, and generate human language.\n",
    "NLP is used in applications like sentiment analysis, translation, and chatbots.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is deep learning?\",\n",
    "        \"answer\": \"\"\"Deep learning is an advanced branch of machine learning that uses neural networks with multiple layers to model complex patterns in data.\n",
    "It has been particularly effective in image and speech recognition, NLP, and autonomous systems.\n",
    "Deep learning typically requires large datasets and high computational power.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is a CPU?\",\n",
    "        \"answer\": \"\"\"The CPU (Central Processing Unit) is the primary component of a computer responsible for executing instructions and performing calculations.\n",
    "It acts as the brain of the computer, coordinating tasks between hardware and software.\n",
    "A faster CPU improves overall system performance.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is RAM?\",\n",
    "        \"answer\": \"\"\"RAM (Random Access Memory) is volatile memory used by a computer to temporarily store data that is actively used.\n",
    "It allows the CPU quick access to information and supports multitasking.\n",
    "The amount of RAM impacts the system's ability to run multiple applications simultaneously.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is a hard drive?\",\n",
    "        \"answer\": \"\"\"A hard drive is a storage device that uses magnetic storage to store and retrieve digital information.\n",
    "It provides long-term storage and retains data even when the computer is powered off.\n",
    "Modern hard drives include traditional HDDs and faster SSDs.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is a network?\",\n",
    "        \"answer\": \"\"\"A network is a collection of interconnected computers that share resources and communicate with each other.\n",
    "Networks can range from small local area networks (LANs) to global wide area networks (WANs).\n",
    "They are essential for data sharing and communication.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the internet?\",\n",
    "        \"answer\": \"\"\"The internet is a global network of interconnected computers that communicate using standardized protocols.\n",
    "It enables the sharing of information, communication, and access to a wide range of online services.\n",
    "The internet has revolutionized how we access and distribute information.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is a database?\",\n",
    "        \"answer\": \"\"\"A database is an organized collection of data that is stored electronically and structured for efficient access.\n",
    "It allows users to store, query, and manipulate data using structured query languages (SQL).\n",
    "Databases are fundamental to various applications from business operations to research.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is a web browser?\",\n",
    "        \"answer\": \"\"\"A web browser is a software application that allows users to access and navigate the internet.\n",
    "It retrieves, interprets, and displays web pages, making online content accessible.\n",
    "Popular browsers include Chrome, Firefox, and Safari.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is cloud computing?\",\n",
    "        \"answer\": \"\"\"Cloud computing is the delivery of computing services such as storage, processing, and software over the internet.\n",
    "It allows users to access and use these resources on demand without maintaining physical hardware.\n",
    "Cloud computing offers scalability, flexibility, and cost efficiency.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is cybersecurity?\",\n",
    "        \"answer\": \"\"\"Cybersecurity is the practice of protecting computers, networks, and data from cyber attacks, unauthorized access, and damage.\n",
    "It involves implementing technologies, processes, and policies to safeguard digital assets.\n",
    "Cybersecurity is crucial for maintaining privacy and system integrity in a digital world.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is an API?\",\n",
    "        \"answer\": \"\"\"An API (Application Programming Interface) is a set of protocols and tools that enables different software applications to communicate with each other.\n",
    "It defines methods and data formats for requesting and exchanging information between systems.\n",
    "APIs are integral in web development, mobile applications, and cloud services.\"\"\"\n",
    "    },\n",
    "]\n",
    "\n",
    "############################################\n",
    "# 4. Generate predictions & compute ROUGE\n",
    "############################################\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "for sample in sample_data:\n",
    "    question = sample[\"question\"]\n",
    "    gold_answer = sample[\"answer\"]\n",
    "\n",
    "    # Generate the model's answer (with max_length set to 512 tokens)\n",
    "    model_answer = generate_answer(question)\n",
    "    predictions.append(model_answer)\n",
    "    references.append(gold_answer)\n",
    "\n",
    "results = rouge.compute(predictions=predictions, references=references)\n",
    "\n",
    "############################################\n",
    "# 5. Print out the average ROUGE results\n",
    "############################################\n",
    "\n",
    "print(\"ROUGE Results (averaged over 20 samples):\")\n",
    "for metric_name, score_value in results.items():\n",
    "    print(f\"{metric_name}: {score_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6510e2cf-6929-4981-9033-9619a60343f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!streamlit run app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b637f5d-9db3-4766-8c5f-29e928ecaf1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97a4e0c-7f55-41af-8a02-0afc62f3f5ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4aeabfd-7fd1-47ab-9603-2826f6085e3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bb8509-1de6-4acc-8af9-bbc97789f558",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b262e7-2fd5-4ad9-b99b-6f5549a46763",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1eaa532-f785-40d1-92d7-03c9a731b99f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac7cb8c-3a15-42cb-b3dc-b002652589cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02016f10-26ee-4a1f-a33f-059707993e71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353d6b1a-8780-4f4e-9f4a-7f52c1e9d459",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93ac4a3-d947-4202-bc20-9a3a50698329",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b453cbdd-5830-496d-a709-ad759eea1d96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1be79d9-7f72-49a6-8e62-c2c2c7dde1ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9cca39-0d94-4c59-b2c6-077103413761",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e453799e-9a69-4fc7-8286-052bd9445a7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
